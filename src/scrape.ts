// my-scrape-project/src/scrape.ts

import type { Browser, Page, Locator } from 'playwright'; 
import { chromium } from 'playwright';
import { insertOrUpdateSupabase } from './supabase'; 

export interface ScrapedJobData {
Â  Â  title: string;
Â  Â  companyName: string; 
Â  Â  url: string;
Â  Â  salary: string;
Â  Â  siteUrl: string; 
}

const BASE_URL: string = 'https://www.workingnomads.com'; 
const TARGET_URL: string = `${BASE_URL}/jobs?postedDate=1`; 
const MAX_SCROLL_COUNT = 500; 

const SELECTORS = {
Â  Â  JOB_CONTAINER: '.job-wrapper',
Â  Â  TITLE_URL: 'h4.hidden-xs a',
Â  Â  COMPANY_CONTAINER: '.job-company', 
Â  Â  LIST_SALARY: 'div[ng-show*="model.salary_range"] span.about-job-line-text.ng-binding',
Â  Â  DETAIL_SALARY_A: '.job-details-inner div:has(i.fa-money)', 
Â  Â  DETAIL_SALARY_B: 'div.job-detail-sidebar:has(i.fa-money)',
Â  Â  // DÃœZÆLÄ°Å 1: Selector-u class-dan ID-yÉ™ Ã§eviririk (daha etibarlÄ±)
Â  Â  LIST_PARENT: '#result-list', 
};

async function scrapeDetailPageForSalary(browser: Browser, url: string): Promise<string> {
Â  Â  const detailPage = await browser.newPage();
Â  Â  let salary = 'N/A';

Â  Â  try {
Â  Â  Â  Â  await detailPage.goto(url, { timeout: 40000, waitUntil: 'domcontentloaded' });
Â  Â  Â  Â  const locatorA = detailPage.locator(SELECTORS.DETAIL_SALARY_A).filter({ hasText: '$' }).first();
Â  Â  Â  Â  const locatorB = detailPage.locator(SELECTORS.DETAIL_SALARY_B).filter({ hasText: '$' }).first();
Â  Â  Â  Â  let salaryText: string | null = null;
Â  Â  Â  Â  
Â  Â  Â  Â  try { salaryText = await locatorA.innerText({ timeout: 5000 }); } catch (e) {
Â  Â  Â  Â  Â  Â  try { salaryText = await locatorB.innerText({ timeout: 5000 }); } catch (e) { }
Â  Â  Â  Â  }
Â  Â  Â  Â  
Â  Â  Â  Â  if (salaryText && salaryText.includes('$')) {
Â  Â  Â  Â  Â  Â  const lines = salaryText.split('\n').map(line => line.trim()).filter(line => line.length > 0);
Â  Â  Â  Â  Â  Â  const salaryLine = lines.find(line => line.includes('$'));
Â  Â  Â  Â  Â  Â  salary = salaryLine ? salaryLine : salaryText.trim();
Â  Â  Â  Â  }

Â  Â  } catch (e) {
Â  Â  Â  Â  // Salary tapÄ±lmadÄ±
Â  Â  } finally {
Â  Â  Â  Â  await detailPage.close();
Â  Â  }
Â  Â  return salary;
}

async function extractInitialJobData(wrapper: Locator): Promise<ScrapedJobData> {
Â  Â  
Â  Â  const titleLocator = wrapper.locator(SELECTORS.TITLE_URL).first();
Â  Â  let title = '', relativeUrl = null, url = 'N/A', companyName = 'N/A', salary = 'N/A';
Â  Â  
Â  Â  try {
Â  Â  Â  Â  title = (await titleLocator.innerText({ timeout: 500 })).trim();
Â  Â  Â  Â  relativeUrl = await titleLocator.getAttribute('href');
Â  Â  Â  Â  url = relativeUrl ? `${BASE_URL}${relativeUrl}` : 'N/A';
Â  Â  } catch (e) {
Â  Â  Â  Â  return { title: '', companyName: 'N/A', url: 'N/A', salary: 'N/A', siteUrl: BASE_URL }; 
Â  Â  }

Â  Â  try {
Â  Â  Â  Â  const companyContainerLocator = wrapper.locator(SELECTORS.COMPANY_CONTAINER).first(); 
Â  Â  Â  Â  let rawText = (await companyContainerLocator.innerText({ timeout: 1000 })).trim(); 
Â  Â  Â  Â  let cleanedText = rawText.replace(/\s+/g, ' ').trim(); 
Â  Â  Â  Â  
Â  Â  Â  Â  const lowerCaseName = cleanedText.toLowerCase();
Â  Â  Â  Â  if (cleanedText.length > 2 && 
Â  Â  Â  Â  Â  Â  !lowerCaseName.includes('full-time') && 
Â  Â  Â  Â  Â  Â  !lowerCaseName.includes('remote') &&
Â  Â  Â  Â  Â  Â  !lowerCaseName.includes('jobs')) 
Â  Â  Â  Â  {
Â  Â  Â  Â  Â  Â  companyName = cleanedText;
Â  Â  Â  Â  }

Â  Â  } catch (e) { 
Â  Â  Â  Â  companyName = 'N/A';
Â  Â  }
Â  Â  
Â  Â  if (companyName === 'N/A' || companyName.length < 3) {
Â  Â  Â  Â  const urlParts = url.split('-');
Â  Â  Â  Â  const companyIndex = urlParts.findIndex(part => /^\d{7}$/.test(part)); 
Â  Â  Â  Â  if (companyIndex > 0) {
Â  Â  Â  Â  Â  Â  let guess = urlParts[companyIndex - 1];
Â  Â  Â  Â  Â  Â  companyName = guess.charAt(0).toUpperCase() + guess.slice(1);
Â  Â  Â  Â  }
Â  Â  }
Â  Â  
Â  Â  try {
Â  Â  Â  Â  const salaryLocator = wrapper.locator(SELECTORS.LIST_SALARY).filter({ hasText: '$' }).first();
Â  Â  Â  Â  const salaryText = await salaryLocator.innerText({ timeout: 500 });
Â  Â  Â  Â  if (salaryText.includes('$') && salaryText.length > 5) {
Â  Â  Â  Â  Â  Â  salary = salaryText.trim();
Â  Â  Â  Â  }
Â  Â  } catch (e) { }

Â  Â  return { title, companyName, url, salary, siteUrl: BASE_URL };
}

export async function runScrapeAndGetData() {
Â  Â  
Â  Â  console.log(`\n--- WorkingNomads Scraper iÅŸÉ™ dÃ¼ÅŸdÃ¼ ---`);
Â  Â  console.log(`Naviqasiya edilir: ${TARGET_URL}`);
Â  Â  
Â  Â  const browser: Browser = await chromium.launch({ 
Â  Â  Â  Â  headless: true,
Â  Â  Â  Â  args: [
Â  Â  Â  Â  Â  Â  '--no-sandbox',
Â  Â  Â  Â  Â  Â  '--disable-setuid-sandbox',
Â  Â  Â  Â  Â  Â  '--disable-dev-shm-usage',
Â  Â  Â  Â  Â  Â  '--disable-gpu',
Â  Â  Â  Â  ]
Â  Â  }); Â  Â 
Â  Â  
Â  Â  const page: Page = await browser.newPage();
Â  Â  
Â  Â  await page.setExtraHTTPHeaders({
Â  Â  Â  Â  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
Â  Â  Â  Â  'Accept-Language': 'en-US,en;q=0.9',
Â  Â  Â  Â  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
Â  Â  });
Â  Â  
Â  Â  try {
Â  Â  Â  Â  await page.goto(TARGET_URL, { timeout: 60000 });
Â  Â  Â  Â  // DÃœZÆLÄ°Å 2: ID-yÉ™ gÃ¶rÉ™ gÃ¶zlÉ™yirik vÉ™ vaxtÄ± 60 saniyÉ™yÉ™ artÄ±rÄ±rÄ±q.
Â  Â  Â  Â  await page.waitForSelector(SELECTORS.LIST_PARENT, { timeout: 120000 }); 

Â  Â  Â  Â  let currentJobCount = 0;
Â  Â  Â  Â  let previousCount = 0;
Â  Â  Â  Â  let sameCountIterations = 0; 
Â  Â  Â  Â  
Â  Â  Â  Â  while (currentJobCount < MAX_SCROLL_COUNT && sameCountIterations < 10) { 
Â  Â  Â  Â  Â  Â  await page.evaluate(() => window.scrollTo(0, document.body.scrollHeight));
Â  Â  Â  Â  Â  Â  await page.waitForTimeout(2000); 
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  previousCount = currentJobCount;
Â  Â  Â  Â  Â  Â  currentJobCount = await page.locator(SELECTORS.JOB_CONTAINER).count();
Â  Â  Â  Â  Â  Â  console.log(`-> MÃ¶vcud elan sayÄ±: ${currentJobCount}`);
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  if (currentJobCount === previousCount) {
Â  Â  Â  Â  Â  Â  Â  Â  sameCountIterations++;
Â  Â  Â  Â  Â  Â  } else {
Â  Â  Â  Â  Â  Â  Â  Â  sameCountIterations = 0;
Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  if (sameCountIterations >= 10 && currentJobCount > 0) { 
Â  Â  Â  Â  Â  Â  Â  Â  console.log("âœ… Say dÉ™yiÅŸmir. BÃ¼tÃ¼n mÃ¶vcud elanlar tapÄ±ldÄ±.");
Â  Â  Â  Â  Â  Â  Â  Â  break;
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  }
Â  Â  Â  Â  
Â  Â  Â  Â  console.log(`\n${currentJobCount} elementdÉ™n mÉ™lumat Ã§Ä±xarÄ±lÄ±r...`);
Â  Â  Â  Â  const jobWrappers = await page.locator(SELECTORS.JOB_CONTAINER).all();
Â  Â  Â  Â  
Â  Â  Â  Â  const initialResults: ScrapedJobData[] = [];
Â  Â  Â  Â  
Â  Â  Â  Â  for (let i = 0; i < jobWrappers.length; i++) {
Â  Â  Â  Â  Â  Â  const result = await extractInitialJobData(jobWrappers[i]);
Â  Â  Â  Â  Â  Â  initialResults.push(result);
Â  Â  Â  Â  }
Â  Â  Â  Â  
Â  Â  Â  Â  console.log(`ğŸ“Š ${initialResults.filter(j => j.title.length > 0).length} real elan tapÄ±ldÄ±`);
Â  Â  Â  Â  
Â  Â  Â  Â  const finalResults: ScrapedJobData[] = []; 
Â  Â  Â  Â  
Â  Â  Â  Â  for (const job of initialResults) {
Â  Â  Â  Â  Â  Â  if (job.title.length > 0) {
Â  Â  Â  Â  Â  Â  Â  Â  if (job.salary === 'N/A' && job.url.startsWith(BASE_URL)) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const detailSalary = await scrapeDetailPageForSalary(browser, job.url); 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  job.salary = detailSalary;
Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  finalResults.push(job);
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  }
Â  Â  Â  Â  
Â  Â  Â  Â  const filteredResults = finalResults.filter(job => job.url !== 'N/A');

Â  Â  Â  Â  console.log("\n--- SCRAPING NÆTÄ°CÆLÆRÄ° ---");
Â  Â  Â  Â  console.log(`\nâœ… Yekun NÉ™ticÉ™: ${filteredResults.length} elan Ã§Ä±xarÄ±ldÄ±.`);

Â  Â  Â  Â  await insertOrUpdateSupabase(filteredResults);

Â  Â  Â  Â  return filteredResults; 

Â  Â  } catch (e) {
Â  Â  Â  Â  console.error(`âŒ Æsas XÉ™ta: ${e instanceof Error ? e.message : String(e)}`);
Â  Â  Â  Â  throw e; 
Â  Â  } finally {
Â  Â  Â  Â  await browser.close();
Â  Â  Â  Â  console.log('--- Scraper bitdi ---');
Â  Â  }
}